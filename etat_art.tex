\documentclass[memoire.tex]{subfiles}
\begin{document}
\chapter{Etat de l'art du clustering}
Le Terme clustering fait référence au concept de classification non supervisée faisant elle même d'une plus grande famille qu'est l'apprentissage non supervisé. A l'inverse de l'apprentissage supervisé qui nécessite des données déjà segmentées, le clustering vise quant à lui à déterminer une segmentation du jeu de données étudié. Dans ce cas, l'intervention humaine n'est pas nécessaire étant donné  que l'ordinateur détermine les différentes segmentations sans l'apport de variables cibles fournies à l'algorithme.
L'analyse de cluster permet donc d'identifier des groupes de données relativement homogènes sur la base de leur similarité pour des caractéristiques données ce qui dans notre cas peut par exemple être le type d'emploi occuper en fonction des filières suivies par d'anciens étudiants. Cependant analyser différents profils d'individus peut représenter des difficultés techniques importantes, c'est pourquoi cette première section du document présentera les différentes solutions possibles pouvant apporter une réponse à cette difficulté de catégorisation des parcours.
\section{La préparation des données}
Lorsqu'une segmentation basée sur des clusters est utilisée, il existe plusieurs formes de préparations de données pouvant aider à la formation des différents segments. 
\subsection{La Transformation des variables}
Il existe deux types de transformation : \begin{itemize}
\item La modification de la portée des variables connue en tant que la standardisation des variables.
\item{La modification de la forme de distribution}
\end{itemize}
L'utilisation de la standardisation est motivée par le fait que l'analyse de cluster implique une étude implicite du poids des objets afin de pouvoir se concentrer sur ceux possédant une variance plus élevée. Les méthodes de standardisation les plus communes sont les suivantes \cite{ref9} : \begin{itemize}
\item Multiplication de chaque variable par une différente constante.
\item Utilisation des techniques de réduction de dimensions, qui un processus visant à réduire le nombre de variables aléatoires afin d'obtenir un jeu de variables principal.
\item Multiplier chaque variables par une différente constante afin que chacune d'entre elles aient une portée commune.
\end{itemize}
La modification de la distribution quant à elle est motivée par les mêmes problématiques que dans d'autres secteurs ayant recours aux statistiques qui sont d'extrêmes variations par rapport à ce qui est considéré "normal" dans le cas étudié. Celles-ci entrainent des analyses pouvant induire en erreur. Par conséquent lorsque le poids des variables est modifié le but est d'identifier et supprimer leur longue traine qui correspond à un nombre d'entre elles possédant des valeurs très supérieures ou inférieures à la moyenne.

\section{La mesure de la distance}
Le choix de la méthode de mesure de distance est une étape critique pour les méthodes de clustering, son choix ayant une très forte influence sur le résultat final. En effet, la méthodologie choisie définira comment les similarités de deux éléments sont calculés et influera par conséquent sur la forme des clusters également. Les deux méthodes les plus communes de mesure sont la distance Euclidienne illustrée par la formule suivante (figure1.1) : 

	\begin{figure}[h!]
		\centerline{\includegraphics[scale=0.8]{img/euclidienne_distance.png}}
		\caption{Exemple de dendrogramme}
	\end{figure}
Dans cette méthode, la distance est calculée en effectuant le carré de la somme des carrés des distances entre les variables répondant à un critère donné.\\
La seconde méthode communément utilisée est la distance de Manhattan, appelée également "taxi-distance". Celle-ci, pour un point A et B de coordonnées respectives ($X_{a}$,$Y_{a}$) et ($X_{b}$,$Y_{b}$) est définie de la façon  suivante :\\
\begin{center}
$d(A,B) = [X_{b} - X_{a}] + [Y_{b} - Y_{a}]$\\
\end{center}
À l'inverse de la méthode euclidienne qui pourrait être influencée par des valeurs inhabituelles, le calcul de la distance de Manhattan va s'effectuer selon la différence moyennes entre les dimensions. La présence de valeurs aberrantes impactera le résultat de façon réduite étant donné qu'elle ne sera pas élevée au carré contrairement à la méthode euclidienne, ce qui fait que cette méthode aura tendance à donner le même type de résultat.
\newpage
\section{Les méthodes de clustering}
		\subsection{Le clustering hiérarchique}
Parmis les différents types de clustering existant \cite{ref4}, le premier étudié sera le clustering hiérarchique. Très utilisé comme outil d'analyse de données, l'idée principale de cette méthode est de construire un arbre binaire fusionnant de façon successive les groupes de points similaires. L'un des avantages de cette méthode est tout d'abord l'apport de l'arbre qui permet d'avoir une vision globale des données traitées. De plus, cette méthodologie possède ses propres outils de visualisation qui sont le dendrogramme et la classification double. Le dendrogramme permet d'illustrer l'arrangement des clusters (figure 1.2)\cite{ref8} :
\begin{itemize}
\item la racine de l'arbre est formée par un cluster contenant l'ensemble des objets.
\item chaque nœud de l'arbre constitue un cluster.
\item l'union des objets des nœuds fils correspond aux objets présents dans le nœud racine.
\item les paliers sont indexés en fonction de l'ordre de construction.
\end{itemize}

 Tandis que la classification double est une technique d'exploration de données non-supervisée permettant de segmenter simultanément les lignes et les colonnes d'une matrice. L'autre avantage du clustering hiérarchique est sa facilité d'implémentation dans des algorithmes tel que K-Means en plus de fournir une représentation comme dit précédemment. Afin d'établir un arbre hiérarchique, le clustering hiérarchique à recours à deux méthodes qui sont la méthode agglomérative et la méthode divisive. Un regroupement agglomératif traite chaque objet comme un seul élément qui à chaque étape de l'algorithme est fusionné avec un second objet présentant le plus de similarités en un nouveau cluster de plus grande taille. Ce processus est répété jusqu'à que ce que tous les points soient membre d'un seul et même cluster. À l'inverse d'un regroupement agglomératif qui utilise une approche "bottom-up", les algorithmes divisifs utilisent une approche "top-down". Ces algorithmes débutent ainsi leur traitement à partir de la racine de l'arbre ou tous les objets sont regroupés en un seul cluster. À chaque itération les cluster les plus hétérogènes sont divisés en deux jusqu'à ce que l'ensemble des objets fassent partie de leur propre cluster. Toutefois sa complexité le rend inefficace sur de larges jeux de données \cite{ref7}. De plus, la première injection de données ainsi que l'ordre de celles-ci  à un fort impact sur le résultat final. En outre, il n'est pas possible de défaire ou modifier les étapes précédentes du traitement, c'est-à-dire qu'une fois une instance assignées à un cluster, il n'est plus possible de la déplacée pour effectuer d'éventuelles modifications ou corrections \cite{ref5}. Dans notre cas la base de CV utilisée n'étant pas de taille importante le clustering hiérarchique reste une méthode applicable. Cependant la problématique à résoudre est la gestion des filières intégrant plusieurs domaines tel que la filière MIASHS de Nanterre qui possède une dimension mathématique et une informatique. Les données étant représentées sous forme d'arbre cela entrainerait une répétition au niveau des résultats.
	\begin{figure}[h!]
		\centerline{\includegraphics[scale=0.8]{img/hierarchical_clustering.png}}
		\caption{Exemple de dendrogramme}
	\end{figure}
	
\newpage
\subsection{Le clustering par partionnement}
Le clustering par partionnement contrairement au clustering hiérarchique qui utilise un arbre afin de représenter les différents groupe de données va classifier les différents objets en groupe en fonction de leur similarités. Cependant ce mode de fonctionnement pause un problème concernant le choix de la "bonne représentation" en fonction d'un critère choisi, le but devient alors de recherche une représentation optimale de son critère à travers plusieurs itérations.\cite{ref8} L'algorithme le plus utilisé pour ce type de méthode est K-means qui sera présenté dans la suite de ce document.

 
\section{Les autres types de clustering}
\subsection{Le clustering basé sur des mélanges de modèles}
Cette approche est adoptée lors de l'utilisation d'apprentissage automatique et au traitement de données manquantes ou cachées, par rapport à d'autes approches basées sur des métriques permet de déterminer le nombre de classes nécessaires  ou encore de déterminer le degré d'incertitude \cite{ref10}. Les limites de cette méthode résident principalement dans les limitations entrainées par le type de données utilisées et qu'il est nécessaire de formuler des hypothèses sur la distribution des observations rarement vérifiables dans la réalité.\cite{ref8}

\subsection{Le clustering conceptuel}
Le clustering conceptuel est paradigme d'apprentissage non supervisé ayant fait son apparition durant les années 1980. Cette méthode se différencie du clustering de données classique par le fait qu'elle génère une description de concept pour chaque classes générées. Un autre facteur de différenciation est le fait que les phases de clusterisation et de caractérisation des données ne sont pas indépendantes.\cite{ref11}


\section{Les types de clusters}

Le but du clustering étant de trouver des groupes d'objets présentant des similarités
définies en fonction de l'objectif recherché. Il existe toutefois une multitude de types de cluster qui seront étudiés au sein de cette section chacun avec ses avantages et inconvénients en fonction de notre cas avant de statuer sur le type qui sera utilisé pour le reste de ce document.

\subsection{Well-Separated}
Un cluster "well-separated" est un regroupement de points de telle façon à ce que tous les points faisant parti d'un même cluster présentent de fortes similarités entre eux comparés aux points d'un cluster extérieur(figure 1.3). Si la population de cluster est suffisament bien compartimentée, ce type de cluster permet de faire fonctionner n'importe quelle méthode de clustering de façon efficace.
	\begin{figure}[h!]
		\centerline{\includegraphics[scale=0.8]{img/well_separated.png}}
		\caption{Well separated clusters}
	\end{figure}

\subsection{Prototype-Based}
Un prototype-based cluster est un cluster dont les points qui le constitue sont plus proches ou similaire du prototype définissant le cluster traité que de tout autre prototype définissant d'autres clusters.

\subsection{Graph-Based}
Le graph-based cluster est utilisé dans les cas ou les données peuvent être représenté sous forme de graphe dont les nœuds sont des objets et les liens représentent les connexions entre ceux-ci. Dans cette situation un cluster peut être défini comme un composant connecté,  c'est-à-dire un groupe d'objets liés les uns aux autres au sein du même groupe. En outre ce type de classification permet de visualiser facilement d'importants jeux de données.

\newpage
\subsection{Density-Based}
A travers l'utilisation d'un density-based cluster, le but est de détecter les zones ou les points formant des clusters sont concentrés et celles ou les points sont séparés par des zones vides ou par des zones contenant très peu de points (figure 1.4). Les points ne faisant par partie d'un agrégat sont ici considérés comme du bruit. Ce type de définition est utilisée lorsque les clusters s'entrecroisent ou sont irréguliers.\cite{ref4} Il est également possible d'avoir recours à ce type de classification lorsque du bruit est présent, celui-ci peut former des ponts entre les clusters lorsqu'un autre type de classification est utilisée.
	\begin{figure}[h!]
		\centerline{\includegraphics[scale=0.8]{img/density_cluster.png}}
		\caption{Density based clusters}
	\end{figure}

\newpage
\section{Les algorithmes}
Dans cette section seront décrit les principaux algorithmes utilisés lorsque des techniques de clustering sont employées. Les avantages et inconvénients de chacun seront présentés en fonction du cas présenté dans l'introduction.
\subsection{K-means}

L'algorithme K-means est l'algorithme le plus populaire, celui-ci peut être utiliser dans plusieurs domaines tels que : 
\begin{itemize}
\item Utilisation du clustering dans un contexte de Data Mining.
\item Clustering de documents qui dans notre cas seraient les CV d'anciens étudiants.
\item La segmentation d'un jeu de données en fonction de critères.
\end{itemize}
Celui-ci recherche la meilleure division possible au sein d'un jeux de données \cite{ref5} et possède comme avantage une certaine facilité d'implémentation. Cependant, il impose de savoir le nombre de clusters \textbf{K} souhaités et par conséquent une bonne connaissance des données utilisées. L'une des solutions possibles, lorsqu'un grand jeu de données est utilisé afin de déterminer le nombre de cluster voulu est de lancer l'algorithme avec différentes valeurs et de calculer ensuite la variance entre les résultats obtenus. Celle-ci représente ainsi la somme des distances entre chaque \textit{centroïde}.\\
K-means fonctionne de la façon suivante : 
\begin{itemize}
\item Choix d'un nombre de clusters K
\item Affectation de chaque point au groupe dont il est le plus proche.
\item Itération jusqu'à ce qu'il n'y ait plus de changements au niveau des centroïdes, c'est à dire que ceux-ci ne bougent plus lors des itérations.
\end{itemize}

\subsection{Agglomerative Hierarchical Clustering}

Les techniques de clustering agglomératives partent d'un ensemble de points formant un cluster, par la suite, les deux clusters les plus proches sont fusionnés successivement jusqu'à ce qu'il n'y est plus qu'un seul cluster restant. \cite{ref4}
\newpage
\subsection{DBSCAN}
DBSCAN est un algorithme basé sur le partitionnement de données, celui-ci utilise deux principaux paramètres qui sont la distance minimale entre deux points et le nombre de points minimum devant se trouver dans un rayon donné afin qu'ils soient considérés comme un cluster\cite{ref12}. Le choix d'une bonne mesure de la distance reste critique comme dans toute autre méthode de clustering. En effet, si la valeur choisie est trop petite, une partie des données traites risque d'être considérer des \textit {outliers} c'est à dire des observations peu fréquentes sortant de la norme.\\
l'agorithme DBSCAN sert dans des situations ou l'on cherche à déterminer des structures au sein d'un jeu de données, celui-ci peut être exprimer en peudo-code.(figure 1.5) Du à sa popularité, cet algorithme est souvent directement importé à travers des libraires Python ou R.
	\begin{figure}[h!]
		\centerline{\includegraphics[scale=0.8]{img/dbscan.png}}
		\caption{Algorithme DBSCAN}
	\end{figure}
Cet algorithme offre comme avantages le fait qu'il n'a pas besoin de plus de paramètre que la distance, ici marquée \textit{eps} et le nombre de moins minimum afin de constituer un cluster représenter ici par \textit{minpts}. De plus, celui-ci n'est pas très sensible à l'ordre des données pour son traitement. Cependant, la qualité du résultat est grandement lié au choix d'une bonne mesure de distance. En outre, un degré de compréhension de l'échelle appliquée et des données étudiées est nécessaire afin de pouvoir choisir une mesure de distance pertinente.
\end{document}